决策树

1 决策树基本流程
概念：基于树结构来进行决策，体现人类在面临决策问题时一种很自然的处理机制
具备条件：
每个非叶节点表示一个特征属性测试
每个分支代表这个特征属性在某个值域上的输出
每个叶子节点存放一个类别
每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集
基本算法：
输入： 训练集D={(x1,y1),(x2,y2),⋅,(xm,ym)}
D={(x 
1
​	
 ,y 
1
​	
 ),(x 
2
​	
 ,y 
2
​	
 ),⋅,(x 
m
​	
 ,y 
m
​	
 )};
   属性集
A
=
a
1
,
a
2
,
⋅
,
a
d
A=a 
1
​	
 ,a 
2
​	
 ,⋅,a 
d
​	
 
过程： 函数TreeGenerate(
D
D,
A
A)
(1) 生成结点node
(2) if 
D
D中样本全属于同一类别
C
C then
(3)  将node标记为
C
C类叶节点; return
(4) end if
(5) if 
A
=
∅
A=∅ OR 
D
D中样本在
A
A上取值相同 then
(6)  将node标记为叶结点，其类别标记为
D
D中样本数最多的类；return
(7) end if
(8) 从
A
A中选择最优化分属性
a
∗
a 
∗
​	
 ;
(9) for 
a
∗
a 
∗
​	
 的每一个值
a
∗
v
a 
∗
v
​	
  do
(10)  为node生成一个分支；令
D
v
D 
v
​	
 表示
D
D中在
a
∗
a 
∗
​	
 上取值为
a
∗
v
a 
∗
v
​	
 的样本子集;
(11)   if 
D
v
D 
v
​	
 为空 then
(12)    将分支结点标记为叶结点，其类别标记为
D
D中样本最多的类; return
(13)  else
(14)    以TreeGenerate(
D
v
D 
v
​	
 , 
A
\
{
a
∗
}
A\{a 
∗
​	
 })为分支结点
(15)  end if
(16) end for
输出： 以node为根结点的一棵决策树
决策树构造
当前结点包含的样本全部属于同一类，直接将该结点标记为叶结点，其类别设置该类
当属性集为空，或所有样本在所有属性上取值相同，无法进行划分，将该结点标记为叶结点，其类别设置为其父结点所含样本最多的类别
当前结点包含的样本集合为空，不能划分，将该结点标记为叶结点，其类别设置为其父结点所含样本最多的类别
2 划分选择
2.1 信息增益
信息熵：度量样本集合纯度最常用的一种指标
信息熵定义：
  假定当前样本集合
D
D中第
k
k类样本所占的比例为
p
k
(
k
=
1
,
2
,
…
,
∣
Y
∣
)
p 
k
​	
 (k=1,2,…,∣Y∣)，则
D
D的信息熵表示为
Ent
(
D
)
=
−
∑
k
=
1
∣
Y
∣
p
k
2
p
k
Ent(D)=− 
k=1
∑
∣Y∣
​	
 p 
k
​	
 log 
2
​	
 p 
k
​	
 
Ent
(
D
)
Ent(D)值越小，则
D
D的纯度越高。
信息增益定义：
  假定使用属性
a
a对样本集
D
D进行划分，产生了
V
V个分支节点，
v
v表示其中第
v
v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大，可以计算出划分后相比原始数据集
D
D获得的“信息增益”（information gain）。
Gain
(
D
,
α
)
=
Ent
(
D
)
−
∑
v
=
1
V
∣
D
v
∣
∣
D
∣
Ent
(
D
v
)
Gain(D,α)=Ent(D)− 
v=1
∑
V
​	
  
∣D∣
∣D 
v
 ∣
​	
 Ent(D 
v
 )
信息增益越大，使用属性
a
a划分样本集
D
D的效果越好。
ID3决策树学习算法是以信息增益为准则
2.2 增益率
作用：用于解决属性信息熵为0，或远高于其他属性的信息熵问题
定义：
Gain_ratio
(
D
,
α
)
=
Gain
(
D
,
α
)
IV
(
α
)
Gain_ratio(D,α)= 
IV(α)
Gain(D,α)
​	
 
其中
IV
(
α
)
=
−
∑
v
=
1
V
∣
D
v
∣
∣
D
∣
2
∣
D
v
∣
∣
D
∣
IV(α)=− 
v=1
∑
V
​	
  
∣D∣
∣D 
v
 ∣
​	
 log 
2
​	
  
∣D∣
∣D 
v
 ∣
​	
 
当
α
α属性的取值越多时，
IV
(
α
)
IV(α)值越大
C4.5算法是以增益率为准则
2.3 基尼指数
CART决策树使用“基尼指数”（Gini index）来选择划分属性
作用：表示从样本集
D
D中随机抽取两个样本，其类别标记不一致的概率，因此
Gini
(
D
)
Gini(D)越小越好
定义：
Gini
(
D
)
=
∑
k
=
1
∣
Y
∣
∑
k
′
≠
k
p
k
p
k
′
=
1
−
∑
k
=
1
∣
Y
∣
p
k
2
 
Gini(D)
​	
  
= 
k=1
∑
∣Y∣
​	
  
k 
′
  

​	
 =k
∑
​	
 p 
k
​	
 p 
k 
′
 
​	
 
=1− 
k=1
∑
∣Y∣
​	
 p 
k
2
​	
 
​	
 
属性选择：
使用属性
α
α划分后的基尼指数为：
 Gini_index 
(
D
,
α
)
=
∑
v
=
1
V
∣
D
v
∣
∣
D
∣
Gini
(
D
v
)
 Gini_index (D,α)= 
v=1
∑
V
​	
  
∣D∣
∣D 
v
 ∣
​	
 Gini(D 
v
 )
故选择基尼指数最小的划分属性。
