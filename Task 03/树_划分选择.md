## 2 划分选择

### 2.1 信息增益
- 信息熵：度量样本集合纯度最常用的一种指标
- 信息熵定义：  
&emsp;&emsp;假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,\dots,|\mathcal{Y}|)$，则$D$的信息熵表示为$$\text{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y }| } p_{k} \log_2 p_k$$
$\text{Ent}(D)$值越小，则$D$的纯度越高。
- 信息增益定义：  
&emsp;&emsp;假定使用属性$a$对样本集$D$进行划分，产生了$V$个分支节点，$v$表示其中第$v$个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大，可以计算出划分后相比原始数据集$D$获得的“信息增益”（information gain）。
$$\text{Gain}(D, \alpha)=\text{Ent}(D)-\sum_{v=1}^{V} \frac{|D^{v}|}{|D|} \text{Ent}(D^v)$$
信息增益越大，使用属性$a$划分样本集$D$的效果越好。
- ID3决策树学习算法是以信息增益为准则

### 2.2 增益率
- 作用：用于解决属性信息熵为0，或远高于其他属性的信息熵问题
- 定义：
  $$\text{Gain\_ratio}(D, \alpha) = \frac{\text{Gain}(D, \alpha)} { \text{IV} (\alpha) }$$
  其中$$
\text{IV}(\alpha)=-\sum_{v=1}^V \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}$$当$\alpha$属性的取值越多时，$\text{IV}(\alpha)$值越大
- C4.5算法是以增益率为准则

### 2.3 基尼指数
- CART决策树使用“基尼指数”（Gini index）来选择划分属性
- 作用：表示从样本集$D$中随机抽取两个样本，其类别标记不一致的概率，因此$\text{Gini}(D)$越小越好
- 定义：$$\begin{aligned} \text{Gini}(D) 
&=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k' \neq k} p_k p_{k'} \\
&=1-\sum_{k=1}^{|\mathcal{Y}|} p_k^2 
\end{aligned}$$
- 属性选择：  
使用属性$\alpha$划分后的基尼指数为：
$$\text { Gini\_index }(D, \alpha)=\sum_{v=1}^V \frac{|D^v|}{|D|} \text{Gini}(D^v)$$故选择基尼指数最小的划分属性。
——
